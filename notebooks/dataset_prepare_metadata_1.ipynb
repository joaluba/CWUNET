{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################## IMPORT LIBRARIES ##################\n",
    "\n",
    "import numpy as np\n",
    "import random \n",
    "import pandas as pd\n",
    "import os\n",
    "from os.path import join as pjoin\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################## IMPORT MY MODULES ##################\n",
    "\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "import helpers as hlp\n",
    "import importlib\n",
    "importlib.reload(hlp);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# directory where the data bases are stored\n",
    "datapath=\"/home/ubuntu/guestxr2/home/ubuntu/Data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- SPEECH POOL ------------\n",
    "\n",
    "# create df with paths to speech files\n",
    "speech_dataset_path1 =  pjoin(datapath,'VCTK','wav48_silence_trimmed')\n",
    "speech_dataset_path2 =  pjoin(datapath,'PTDB')\n",
    "\n",
    "# initialize empty list of files:\n",
    "speech_pool = []\n",
    "\n",
    "# fill the list of files with filenames from vctk data base:\n",
    "database=\"VCTK\"\n",
    "for root, dirs, files in os.walk(speech_dataset_path1):\n",
    "    for file in files:\n",
    "        if file.endswith('.flac'):\n",
    "            # decide which split based on a probability \n",
    "            speech_pool.append({'database_speech': database, 'speech_file_path': os.path.join(root, file)})\n",
    "\n",
    "# fill the list of files with filenames from ptdb data base:\n",
    "database=\"PTDB\"\n",
    "for root, dirs, files in os.walk(speech_dataset_path2):\n",
    "    for file in files:\n",
    "        # make sure the correct speech files are used (MIC directory)\n",
    "        if (\"/MIC/\" in root) & (file.endswith('.wav')):\n",
    "            # decide which split based on a probability \n",
    "            speech_pool.append({'database_speech': database, 'speech_file_path': os.path.join(root, file)})  \n",
    "\n",
    "# shuffle order\n",
    "random.shuffle(speech_pool)\n",
    "\n",
    "# list to data frame \n",
    "speech_pool = pd.DataFrame(speech_pool)\n",
    "print(f\"{len(speech_pool)=}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- RIR POOL ------------\n",
    "database=\"synth_rirs_mono\"\n",
    "rir_path=pjoin(datapath,database)\n",
    "\n",
    "# load df with rirs paths and stats (it was generated together with the RIRs - rir_dataset.ipynb):\n",
    "rir_pool=pd.read_csv(pjoin(rir_path,\"rir_info.csv\"),index_col=0)\n",
    "# make a column with a file path that includes current directory \n",
    "rir_pool[\"ir_file_path\"] = rir_pool[\"ir_file_name\"].apply(lambda x: pjoin(rir_path, x))\n",
    "if \"ir_clone_file_name\" in rir_pool.columns: # if the database contained \"cloned\" RIRs (same room, different position)\n",
    "    rir_pool[\"ir_clone_file_path\"] = rir_pool[\"ir_clone_file_name\"].apply(lambda x: pjoin(rir_path, x))\n",
    "\n",
    "rir_pool[\"database_rir\"]=database\n",
    "print(f\"{len(rir_pool)=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------- CREATE METADATA FOR A DATASET I.E. COMBINATIONS OF SPEECH AND RIRS --------\n",
    "from datetime import datetime\n",
    "date_tag = datetime.now().strftime(\"%d-%m-%Y--%H-%M\")\n",
    "# create dataset with 150000 data points, which consists of random combinations of speech, noise and rirs\n",
    "N_datapoints=150000\n",
    "\n",
    "# # sample from noise pool: \n",
    "# df_noise=noise_pool.sample(N_datapoints,replace=True)\n",
    "# # here plan ways to augment noise data set:\n",
    "# random_bool_values = [random.choice([1, -1]) for _ in range(len(df_noise))]\n",
    "# df_noise[\"aug_phase\"]=random_bool_values\n",
    "\n",
    "# sample from speech pool: \n",
    "df_speech=speech_pool.sample(N_datapoints,replace=True)\n",
    "# here plan ways to augment noise data set:\n",
    "random_bool_values = [random.choice([1, -1]) for _ in range(len(df_speech))]\n",
    "df_speech[\"aug_phase\"]=random_bool_values\n",
    "\n",
    "# sample from rir pool: \n",
    "df_rir=rir_pool.sample(N_datapoints,replace=True)\n",
    "\n",
    "# concatenate samples from speech, noise and rir pools\n",
    "# df_ds = pd.concat([df_speech.reset_index(drop=True), df_noise.reset_index(drop=True), df_rir.reset_index(drop=True)], axis=1,ignore_index=False)\n",
    "df_ds = pd.concat([df_speech.reset_index(drop=True), df_rir.reset_index(drop=True)], axis=1,ignore_index=False)\n",
    "\n",
    "df_ds = df_ds.reset_index(drop=True)\n",
    "\n",
    "# randomize snr (only high snrs)\n",
    "# df_ds[\"snr\"]= 200 #np.random.uniform(low=10, high=30, size=len(df_ds))\n",
    "\n",
    "# Create test-train-val split:\n",
    "df_ds.loc[0:N_datapoints*0.8,\"split\"]=\"train\" # 80% training data\n",
    "df_ds.loc[N_datapoints*0.8:N_datapoints*0.9,\"split\"]=\"test\" # 10% testing data\n",
    "df_ds.loc[N_datapoints*0.9:N_datapoints,\"split\"]=\"val\" # 10% validation data\n",
    "\n",
    "# save dataset metadata:\n",
    "# df_ds.to_csv(\"../dataset-metadata/ds_metadata_\" + date_tag + \".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check metadata in combination with a dataset class definition\n",
    "\n",
    "sys.path.append('../src')\n",
    "\n",
    "import helpers as hlp\n",
    "import dataset as ds\n",
    "from IPython.display import Audio\n",
    "\n",
    "importlib.reload(hlp)\n",
    "importlib.reload(ds)\n",
    "\n",
    "config=hlp.load_config(\"../config/basic.yaml\")\n",
    "config[\"df_metadata\"]=\"/home/ubuntu/guestxr2/home/ubuntu/joanna/CWUNET/dataset-metadata/ds1_metadata_example.csv\"\n",
    "config[\"split\"]=\"train\"\n",
    "dataset=ds.DatasetReverbTransfer(config)\n",
    "# get one data sample \n",
    "sContent, sStyle, sTarget, sAnechoContent, sAnechoStyle = dataset[55]\n",
    "\n",
    "# playback for the data sample\n",
    "audios=[sContent, sStyle, sTarget, sAnechoContent]\n",
    "names=[\"sContent\", \"sStyle\", \"sTarget\", \"sAnechoContent\"]\n",
    "\n",
    "for i,audio in enumerate(audios):\n",
    "    print(names[i])\n",
    "    audio=audio.squeeze(0).cpu()\n",
    "    display(Audio(audio,rate=48e3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
